{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Amit_Kumar_16110011.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPar0kiKZ5nG",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jYbYi1WZ_9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "fb28bb7f-8dd9-45f4-b56d-2d0fae7c38ba"
      },
      "source": [
        "#ALL HEADER FILES\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,Flatten, Dense, Embedding, RNN, Conv1D, BatchNormalization, MaxPooling1D, Activation, Dropout, concatenate, Lambda\n",
        "from keras import optimizers\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import optimizers\n",
        "import pickle as p\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "from bs4 import BeautifulSoup  \n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "datapath = '/content/gdrive/My Drive/Colab Notebooks/Assignment_3/'\n",
        "modelpath = '/content/gdrive/Shared drives/NLP/Project_Files/Models/EmotionClassifier'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLa3P57AJ-lM",
        "colab_type": "text"
      },
      "source": [
        "Now we will Read Train and Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86-HR_XXMZPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(datapath+\"train.txt\",\"r\") as traintext:\n",
        "  train_file=traintext.read()\n",
        "\n",
        "with open(datapath+\"test.txt\",\"r\") as testtext:\n",
        "  test_file=testtext.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRkNpdpAL-Cu",
        "colab_type": "text"
      },
      "source": [
        "Pre_Processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruOaE4GbMOGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_en=stopwords.words('english')\n",
        "exception=['not','never', 'no','ever','very','nothing','really','extremely']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-vzNaUz_7yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emodict = {0:'amusement', 1:'satisfaction', 2:'optimism', 3:'pride in achievement', 4:'contentment',\n",
        "           5:'anger', 6:'fear', 7:'disgust', 8:'sadness', 9:'contempt'}\n",
        "#emotion_label = {\n",
        "#\t\t\"fear\" : ['😳', '😰', '😨', '😬'],\n",
        "#\t\t\"anger\" : ['😱', '😡', '😤', '😒', '😑', '😠'],\n",
        "#\t\t\"disgust\" :['😭', '😩', '🤦', '😯', '🤢', '😖'],\n",
        "#\t\t\"sadness\" :['😫', '😭', '😞'] ,\n",
        "#\t\t\"contempt\" :['😔', '💔', '😏'],\n",
        "#\t\t\"amusement\" :['😲'],\n",
        "#   \"pride in achievement\" :['👌', '👏', '👍'],\n",
        "#    \"satisfaction\" :['😊', '🙏', '😔', '😎'],\n",
        "#    \"optimism\" :['💪', '😎', '🛐'],\n",
        "#    \"contentment\" :['😇', '😘', '😂', '😁', '💃'],\n",
        "#    \"none\": ['none']  \n",
        "#        }\n",
        "\n",
        "emoji_label={\n",
        "    \"negative\":['😳', '😰', '😨', '😬','😱', '😡', '😤', '😒', '😑', '😠','😭', '😩', '🤦', '😯', '🤢', '😖', '😫', '😭', '😞','😔', '💔', '😏' ],# fear+anger+disgust+sadness+contempt\n",
        "    \"neutral\":['😊', '🙏'],#satisfaction\n",
        "    \"positive\":['😇', '😘', '😂', '😁', '💃','💪', '😎', '🛐','👌', '👏', '👍','😲']#optimmism + contentment\n",
        "}\n",
        "emoji=['😳', '😰', '😨', '😬','😱', '😡', '😤', '😒', '😑', '😠','😭', '😩', '🤦', '😯', '🤢', '😖', '😫', '😭', '😞','😔', '💔', '😏', '😊', '🙏','😇', '😘', '😂', '😁', '💃','💪',\n",
        "       '😎', '🛐','👌', '👏', '👍','😲']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42VMlYEhr4dK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "e09438a6-04ca-4019-83e9-1c8be4518f64"
      },
      "source": [
        "!sudo pip install indic_transliteration -U\n",
        "data = 'idm adbhutam'\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
        "print(transliterate(data, sanscript.HK, sanscript.DEVANAGARI))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic_transliteration\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/d0/bb68ea6a9a473947af76f70d6e43936131e4f56165e5c68bacc73a36584f/indic_transliteration-1.8.9-py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.6MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 11.2MB/s \n",
            "\u001b[?25hCollecting splinter\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/33/6d6e3abeb0fa47284f2d3b74de7f72018a3af0267b0752b9e7c66175c99e/splinter-0.11.0.tar.gz\n",
            "Collecting backports.functools-lru-cache\n",
            "  Downloading https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl\n",
            "Collecting selenium>=3.141.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from splinter->indic_transliteration) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.141.0->splinter->indic_transliteration) (1.24.3)\n",
            "Building wheels for collected packages: splinter\n",
            "  Building wheel for splinter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for splinter: filename=splinter-0.11.0-cp36-none-any.whl size=30144 sha256=d695f974a8d6182e3e35c95d4a8774ec75e735b6e8a941f2457678a722fa78db\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/e1/35/4809427c48cb88853ddc1e701e9c5629bf192ae9324f2d0042\n",
            "Successfully built splinter\n",
            "Installing collected packages: regex, selenium, splinter, backports.functools-lru-cache, indic-transliteration\n",
            "Successfully installed backports.functools-lru-cache-1.6.1 indic-transliteration-1.8.9 regex-2019.11.1 selenium-3.141.0 splinter-0.11.0\n",
            "इद्म् अद्भुतम्\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuvupCMeM6W4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34b764fc-07ef-4ee7-c4d6-4d0eb03b839d"
      },
      "source": [
        "train_id, train_text, train_label, train_c_h, train_c_e, train_c_o=[],[],[],[],[],[]\n",
        "\n",
        "for tweet in train_file.split('\\n\\n'):\n",
        "  lines=tweet.split('\\n')\n",
        "  try:\n",
        "    train_label.append(lines[0].split()[2])\n",
        "    train_id.append(lines[0].split()[1])\n",
        "  except IndexError:\n",
        "    del train_id[-1]\n",
        "    continue\n",
        "  temp_c,temp_c_h,temp_c_e,temp_c_o=[],[],[],[]\n",
        "  for line in lines[1:]:\n",
        "    words=line.split('\\t')\n",
        "    words[0]=words[0].lower()\n",
        "    if(words[1]!='0'):\n",
        "      words[0]=re.sub('[\\W_]+','',words[0])\n",
        "    if words[1]=='Eng' and words[0] in remove_en and words[0] not in exception:\n",
        "      continue\n",
        "    if 'http' in words[0]:\n",
        "      continue\n",
        "    temp_c.append(words[0])\n",
        "    if words[1]== 'Eng':\n",
        "      temp_c_e.append(words[0])\n",
        "    if words[1]== 'Hin':\n",
        "      temp_c_h.append(words[0])\n",
        "    elif words[1]== 'O':\n",
        "      if(words[0] in emoji):\n",
        "        temp_c_o.append(words[0])\n",
        "      \n",
        "  if temp_c==[]:\n",
        "    continue\n",
        "  train_text.append(temp_c)\n",
        "  train_c_h.append(temp_c_h)\n",
        "  train_c_e.append(temp_c_e)\n",
        "  train_c_o.append(temp_c_o)\n",
        "\n",
        "\n",
        "print(len(train_text))\n",
        "  \n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpVXwt7xTCU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c106064-e3af-40a2-c176-7e4493c6fede"
      },
      "source": [
        "test_id, test_text, test_label, test_c_h, test_c_e, test_c_o=[],[],[],[],[],[]\n",
        "\n",
        "for tweet in test_file.split('\\n\\n'):\n",
        "  lines=tweet.split('\\n')\n",
        "  try:\n",
        "    test_label.append(lines[0].split()[2])\n",
        "    test_id.append(lines[0].split()[1])\n",
        "  except IndexError:\n",
        "    del test_id[-1]\n",
        "    continue\n",
        "  temp_c,temp_c_h,temp_c_e,temp_c_o=[],[],[],[]\n",
        "  for line in lines[1:]:\n",
        "    words=line.split('\\t')\n",
        "    words[0]=words[0].lower()\n",
        "    if(words[1]!='0'):\n",
        "      #words[0]=re.sub('[\\W_]+','',words[0])\n",
        "      words[0]=words[0]\n",
        "    if words[1]=='Eng' and words[0] in remove_en and words[0] not in exception:\n",
        "      continue\n",
        "    if 'http' in words[0]:\n",
        "      continue\n",
        "    temp_c.append(words[0])\n",
        "    if words[1]== 'Eng':\n",
        "      temp_c_e.append(words[0])\n",
        "    if words[1]== 'Hin':\n",
        "      temp_c_h.append(words[0])\n",
        "    elif words[1]== 'O':\n",
        "      if(words[0] in emoji):\n",
        "        temp_c_o.append(words[0])\n",
        "  if temp_c==[]:\n",
        "    continue\n",
        "  test_text.append(temp_c)\n",
        "  test_c_h.append(temp_c_h)\n",
        "  test_c_e.append(temp_c_e)\n",
        "  test_c_o.append(temp_c_o)\n",
        "\n",
        "\n",
        "print(len(test_text))\n",
        "  \n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnKf-oBRUzCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6580ed25-0eab-486b-86fc-da148798fe81"
      },
      "source": [
        "train_tweet= [' '.join(i) for i in train_text]\n",
        "test_tweet= [' '.join(i) for i in test_text]\n",
        "\n",
        "train_tweet_dict, test_tweet_dict={},{}\n",
        "\n",
        "train_tweet_dict['hin']= [' '.join(i) for i in train_c_h]\n",
        "train_tweet_dict['eng']= [' '.join(i) for i in train_c_e]\n",
        "\n",
        "test_tweet_dict['hin']= [' '.join(i) for i in test_c_h]\n",
        "test_tweet_dict['eng']= [' '.join(i) for i in test_c_e]\n",
        "\n",
        "train_tweet_dict['o']= [' '.join(i) for i in train_c_o]\n",
        "test_tweet_dict['o']= [' '.join(i) for i in test_c_o]\n",
        "\n",
        "np.unique(train_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['negative', 'neutral', 'positive'], dtype='<U8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAWhWM4KcNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "44f43ba5-afae-494e-c2fa-a10e7c75db42"
      },
      "source": [
        "train_tweet_dict['eng'][:10]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pakistan israel ko',\n",
              " 'co oxf8tr3bly',\n",
              " 'co prnomskkn1',\n",
              " 'atheist krishna jcb full trend',\n",
              " 'loksabha bjp ko without co shtbwcb7fm',\n",
              " 'noirnaveed angelahana6 cricketworldcup bhosdike',\n",
              " 'bhaijan father son pic entire promotions mashallah s2xhwu6lud',\n",
              " 'manojgajjar111 tumhara islea google co bxueug3xsn',\n",
              " 'nolo weni weekend',\n",
              " 'aimim sachins40805591 lage faad co gha9dwnz6u']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1RqYRxXPIob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b11e809d-8253-449d-82a3-d847477c70d5"
      },
      "source": [
        "# MODEL LOADED- EMOJI2VEC and WORD2VEC\n",
        "import gensim.models as gsm\n",
        "e2v = gsm.KeyedVectors.load_word2vec_format('/content/gdrive/Shared drives/NLP/emoji_adding/emoji2vec.bin', binary=True)\n",
        "model = gsm.KeyedVectors.load_word2vec_format('/content/gdrive/Shared drives/NLP/emoji_adding/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCF4y0WMmfv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6a58be93-2dbb-48b4-bcfc-1f9c1c37d9fb"
      },
      "source": [
        "# Model For hindi Embeddings\n",
        "model_h = gsm.Word2Vec.load('/content/gdrive/My Drive/Colab Notebooks/Assignment_3/hi.bin')\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKv4aFf0sWWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "8283e0a8-c9d8-44d0-8091-ffc41f5e1773"
      },
      "source": [
        "data ='bilkula'\n",
        "data=sanscript.SCHEMES[sanscript.OPTITRANS].to_lay_indian(data)\n",
        "word= transliterate(data, sanscript.HK, sanscript.DEVANAGARI)\n",
        "model_h[word]"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-f2815dfc18bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msanscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCHEMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msanscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTITRANS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lay_indian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVANAGARI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'बिल्कुल्प्' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqyLIDIeIHrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "baa5224c-8f36-4b82-c286-abd73462dd40"
      },
      "source": [
        "from numpy import *\n",
        "negative_word_histogram,positive_word_histogram,neutral_word_histogram=[],[],[]\n",
        "negative_tweet_e, positive_tweet_e, neutral_tweet_e=[],[],[]\n",
        "negative_tweet_h, positive_tweet_h, neutral_tweet_h=[],[],[]\n",
        "negative_tweet_o, positive_tweet_o, neutral_tweet_o=[],[],[]\n",
        "train_tweet_embeddings=[] #eng+hindi+emoji\n",
        "count = 0\n",
        "tot = 0\n",
        "for i in range(len(train_label)):\n",
        "  temp=[]\n",
        "  if(train_label[i] ==\"negative\"):\n",
        "  # NEGATIVE  \n",
        "    tot += 1\n",
        "    words_e = train_tweet_dict['eng'][i].split(\" \")\n",
        "    vector_list_e = [model[word] for word in words_e if word in model.vocab]\n",
        "    j = 0\n",
        "    while j < len(vector_list_e):\n",
        "      if sum(np.isnan(vector_list_e[j]))>0:\n",
        "        del vector_list_e[j]\n",
        "      else:\n",
        "        j += 1\n",
        "    # vector_list_e = where(isnan(vector_list_e), 0, vector_list_e)\n",
        "    # where_are_NaNs = isnan(vector_list_e)\n",
        "    # vector_list_e[where_are_NaNs] = 0\n",
        "    avg_vector_e = np.nanmean(vector_list_e,axis=0)\n",
        "    if sum(np.isnan(avg_vector_e)) == 0:\n",
        "      negative_tweet_e.append(avg_vector_e)\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    words_h = train_tweet_dict['hin'][i].split(\" \")\n",
        "    vector_list_h = np.array([model[word] for word in words_h if word in model.vocab])\n",
        "    vector_list_h = where(isnan(vector_list_h), 0, vector_list_h)\n",
        "    where_are_NaNs = isnan(vector_list_h)\n",
        "    vector_list_h[where_are_NaNs] = 0\n",
        "    avg_vector_h = np.average(vector_list_h,axis=0)\n",
        "    negative_tweet_h.append(avg_vector_h)\n",
        "    \n",
        "    words_o = train_tweet_dict['o'][i].split(\" \")\n",
        "    vector_list_o = np.array([e2v[word] for word in words_o if word in model.vocab])\n",
        "    vector_list_o = where(isnan(vector_list_o), 0, vector_list_o)\n",
        "    where_are_NaNs = isnan(vector_list_o)\n",
        "    vector_list_o[where_are_NaNs] = 0\n",
        "    avg_vector_o = np.average(vector_list_o,axis=0)\n",
        "    negative_tweet_o.append(avg_vector_o)\n",
        "    temp.append(avg_vector_e)\n",
        "    temp.append(avg_vector_h)\n",
        "    #temp.append(avg_vector_o)\n",
        "    temp = np.average(temp,axis=0)\n",
        "  if (train_label[i]==\"neutral\"):\n",
        "    words_e = train_tweet_dict['eng'][i].split(\" \")\n",
        "    vector_list_e = np.array([model[word] for word in words_e if word in model.vocab])\n",
        "    vector_list_e = where(isnan(vector_list_e), 0, vector_list_e)\n",
        "    where_are_NaNs = isnan(vector_list_e)\n",
        "    vector_list_e[where_are_NaNs] = 0\n",
        "    avg_vector_e = np.average(vector_list_e,axis=0)\n",
        "    neutral_tweet_e.append(avg_vector_e)\n",
        "    \n",
        "    words_h = train_tweet_dict['hin'][i].split(\" \")\n",
        "    vector_list_h = np.array([model[word] for word in words_h if word in model.vocab])\n",
        "    vector_list_h = where(isnan(vector_list_h), 0, vector_list_h)\n",
        "    where_are_NaNs = isnan(vector_list_h)\n",
        "    vector_list_h[where_are_NaNs] = 0\n",
        "    avg_vector_h = np.average(vector_list_h,axis=0)\n",
        "    neutral_tweet_h.append(avg_vector_h)\n",
        "    \n",
        "    words_o = train_tweet_dict['o'][i].split(\" \")\n",
        "    vector_list_o = np.array([e2v[word] for word in words_o if word in model.vocab])\n",
        "    vector_list_o = where(isnan(vector_list_o), 0, vector_list_o)\n",
        "    where_are_NaNs = isnan(vector_list_o)\n",
        "    vector_list_o[where_are_NaNs] = 0\n",
        "    avg_vector_o = np.average(vector_list_o,axis=0)\n",
        "    neutral_tweet_o.append(avg_vector_o)\n",
        "    temp.append(avg_vector_e)\n",
        "    temp.append(avg_vector_h)\n",
        "    #temp.append(avg_vector_o)\n",
        "    temp = np.average(temp,axis=0)\n",
        "\n",
        "  if(train_label[i]==\"positive\"):\n",
        "    words_e = train_tweet_dict['eng'][i].split(\" \")\n",
        "    vector_list_e = np.array([model[word] for word in words_e if word in model.vocab])\n",
        "    vector_list_e = where(isnan(vector_list_e), 0, vector_list_e)\n",
        "    where_are_NaNs = isnan(vector_list_e)\n",
        "    vector_list_e[where_are_NaNs] = 0\n",
        "    avg_vector_e = np.average(vector_list_e,axis=0)\n",
        "    positive_tweet_e.append(avg_vector_e)\n",
        "    \n",
        "    words_h = train_tweet_dict['hin'][i].split(\" \")\n",
        "    vector_list_h = np.array([model[word] for word in words_h if word in model.vocab])\n",
        "    vector_list_h = where(isnan(vector_list_h), 0, vector_list_h)\n",
        "    where_are_NaNs = isnan(vector_list_h)\n",
        "    vector_list_h[where_are_NaNs] = 0\n",
        "    avg_vector_h = np.average(vector_list_h,axis=0)\n",
        "    positive_tweet_h.append(avg_vector_h)\n",
        "    \n",
        "    words_o = train_tweet_dict['o'][i].split(\" \")\n",
        "    vector_list_o = np.array([e2v[word] for word in words_o if word in model.vocab])\n",
        "    vector_list_o = where(isnan(vector_list_o), 0, vector_list_o)\n",
        "    where_are_NaNs = isnan(vector_list_o)\n",
        "    vector_list_o[where_are_NaNs] = 0\n",
        "    avg_vector_o = np.average(vector_list_o,axis=0)\n",
        "    positive_tweet_o.append(avg_vector_o)\n",
        "\n",
        "    temp.append(avg_vector_e)\n",
        "    temp.append(avg_vector_h)\n",
        "    #temp.append(avg_vector_o)\n",
        "    temp = np.average(temp,axis=0)\n",
        "    \n",
        "  train_tweet_embeddings.append(temp)\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: Mean of empty slice\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffS8v-7XdoMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70971c50-a33a-4703-d0af-e7e6048dc88f"
      },
      "source": [
        "from numpy import *\n",
        "tweet_ = {}\n",
        "for label in ['negative', 'neutral', 'positive']:\n",
        "  tweet_[label] = {}\n",
        "  for key in ['eng', 'hin', 'o']:\n",
        "    tweet_[label][key] = []\n",
        "\n",
        "# negative_tweet_e, positive_tweet_e, neutral_tweet_e=[],[],[]\n",
        "# negative_tweet_h, positive_tweet_h, neutral_tweet_h=[],[],[]\n",
        "# negative_tweet_o, positive_tweet_o, neutral_tweet_o=[],[],[]\n",
        "train_tweet_embeddings=[] #eng+hindi+emoji\n",
        "\n",
        "for i in range(len(train_label)):\n",
        "  temp=[]\n",
        "  for key in ['eng', 'hin', 'o']:\n",
        "    words = train_tweet_dict[key][i].split(\" \")\n",
        "    vector_list = [model[word] for word in words if word in model.vocab]\n",
        "    j = 0\n",
        "    while j < len(vector_list):\n",
        "      if sum(np.isnan(vector_list[j]))>0:\n",
        "        del vector_list[j]\n",
        "      else:\n",
        "        j += 1\n",
        "    avg_vector = np.nanmean(vector_list,axis=0)\n",
        "    if sum(np.isnan(avg_vector)) == 0:\n",
        "      tweet_[train_label[i]][key].append(avg_vector)\n",
        "    temp.append(avg_vector)\n",
        "  temp = np.average(temp,axis=0)\n",
        "  train_tweet_embeddings.append(temp)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: Mean of empty slice\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuD7Gxp1f9YY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "163a4df2-b4cc-4d3a-b530-474ae0ace22d"
      },
      "source": [
        "tweet_['negative']['eng'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8CLyy2vcBe1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecff90ef-3c15-4b9d-9d0e-e82635f87ff2"
      },
      "source": [
        "count, tot"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(171, 4459)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCImj8yaDhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "da987a1f-7996-4a4f-e931-d404b1dea1de"
      },
      "source": [
        "# a = np.array([[1,1,1], [2,2,2],[3,3,3]]\n",
        "#              )\n",
        "# del a[0,:]\n",
        "# a"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7fbd42914a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a = np.array([[1,1,1], [2,2,2],[3,3,3]]\n\u001b[1;32m      2\u001b[0m              )\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot delete array elements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yokhljirKHDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tweet_embeddings=[]\n",
        "for i in range(len(test_tweet)):\n",
        "  words = test_tweet[i].split(\" \")\n",
        "  vector_list = np.array([model[word] for word in words if word in model.vocab])\n",
        "  avg_vector = np.average(vector_list,axis=0)\n",
        "  test_tweet_embeddings.append(avg_vector)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHaACnPuEJvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1552f3c6-24e3-4227-9c7a-4f1d3d77c936"
      },
      "source": [
        "len(train_tweet_embeddings[1])"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HUPuffDQLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a8b410c1-2017-4d1c-d768-0502af554344"
      },
      "source": [
        "train_tweet[:2]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' adilnisarbutt pakistan ka ghra tauq pakistan israel ko tasleem nahein kerta isko palestine kehta he  occupied palestine',\n",
              " 'madarchod mulle ye mathura me nahi dikha tha jab mullo ne hindu ko iss liye mara ki vo lasse ki paise mag liye    co  oxf8tr3bly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iLtcCB2y18p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "753c26c2-3855-4ae9-dfec-c7136cedbbce"
      },
      "source": [
        "avg_={}\n",
        "for label in ['negative','neutral','positive']:\n",
        "  avg_[label]={}\n",
        "  for key in ['eng','hin','o']:\n",
        "    avg_[label][key]=np.nanmean(tweet_[label][key],axis=0)\n",
        "    print(sum(avg_[label][key]))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.35712773\n",
            "-2.3616002\n",
            "nan\n",
            "-0.4074724\n",
            "-2.0607688\n",
            "nan\n",
            "-0.297975\n",
            "-1.7554432\n",
            "nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K_MIp9xhz8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "similarity=[]\n",
        "for i in range(len(train_tweet_embeddings)):\n",
        "  temp=[]\n",
        "  for label in ['negative','neutral','positive']:\n",
        "    for key in ['eng','hin']:\n",
        "      distance = scipy.spatial.distance.cosine(avg_[label][key],train_tweet_embeddings[i])\n",
        "      temp.append(distance)\n",
        "  similarity.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBn-7da5kK1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_test=[]\n",
        "for i in range(len(test_tweet_embeddings)):\n",
        "  temp=[]\n",
        "  for label in ['negative','neutral','positive']:\n",
        "    for key in ['eng','hin']:\n",
        "      distance = scipy.spatial.distance.cosine(avg_[label][key],test_tweet_embeddings[i])\n",
        "      temp.append(distance)\n",
        "  similarity_test.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdLSGTzGjoKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3300862e-60e7-444d-9bde-0a0e30c6c5a4"
      },
      "source": [
        "similarity=np.array(similarity)\n",
        "similarity_test=np.array(similarity_test)\n",
        "\n",
        "similarity.shape, similarity_test.shape\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15131, 6), (1869, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Rv6QlRlIbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_final = []\n",
        "add_shape = similarity[0].shape\n",
        "train_indexes = []\n",
        "new_Y_train = []\n",
        "for i in range(len(similarity)):\n",
        "  if (np.sum(np.isnan(similarity[i,:])) == 0):\n",
        "    train_final.append(similarity[i])\n",
        "    new_Y_train.append(Y_train[i])\n",
        "  else:\n",
        "    train_indexes.append(i)\n",
        "\n",
        "test_final = []\n",
        "add_shape = similarity_test[0].shape\n",
        "test_indexes = []\n",
        "new_Y_test = []\n",
        "for i in range(len(similarity_test)):\n",
        "  if (np.sum(np.isnan(similarity_test[i,:])) == 0):\n",
        "    test_final.append(similarity_test[i])\n",
        "    new_Y_test.append(Y_test[i])\n",
        "  else:\n",
        "    test_indexes.append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dJGt7Symtp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4df27eed-e420-48da-b020-d0a5c34cf9b9"
      },
      "source": [
        "np.array(train_final).shape\n",
        "len(train_indexes)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_7PE7cNkfl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "1d280c62-41b7-46dd-bdfd-2d252beec001"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model=DecisionTreeClassifier()\n",
        "model.fit(train_final,new_Y_train)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-bf4d6a3e7077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_Y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkQEuGMwj_dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w25yMbvrhtUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_train_tweet_embeddings = np.zeros((len(train_tweet_embeddings), 300))\n",
        "\n",
        "for i in range(len(train_tweet_embeddings)):\n",
        "  np_train_tweet_embeddings[i,:] = np.array(train_tweet_embeddings[i])\n",
        "\n",
        "np_train_tweet_embeddings.shape\n",
        "\n",
        "np_test_tweet_embeddings = np.zeros((len(test_tweet_embedding), 300))\n",
        "\n",
        "for i in range(len(test_tweet_embedding)):\n",
        "  np_test_tweet_embeddings[i,:] = np.array(test_tweet_embedding[i])\n",
        "\n",
        "np_train_tweet_embeddings.shape\n",
        "np_test_tweet_embeddings.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPHdAmQxHRae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AVG HINDI+ENGLISH - P, N, NU\n",
        "avg_positive_tweet=[]\n",
        "avg_positive_tweet.append(positive_tweet_e)\n",
        "avg_positive_tweet.append(positive_tweet_h)\n",
        "avg_positive_tweet= np.average(avg_positive_tweet, axis=0)\n",
        "\n",
        "avg_negative_tweet=[]\n",
        "avg_negative_tweet.append(negative_tweet_e)\n",
        "avg_negative_tweet.append(negative_tweet_h)\n",
        "avg_negative_tweet= np.average(avg_negative_tweet, axis=0)\n",
        "\n",
        "\n",
        "avg_neutral_tweet=[]\n",
        "avg_neutral_tweet.append(neutral_tweet_e)\n",
        "avg_neutral_tweet.append(neutral_tweet_h)\n",
        "avg_neutral_tweet= np.average(avg_neutral_tweet, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwRKFIVgRCpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d9ffba6-6cc6-4725-8db3-4cfd7a274c70"
      },
      "source": [
        "#print(type(train_tweet_embeddings))\n",
        "#print(type(train_tweet_embeddings[0]))\n",
        "np_train_tweet_embeddings = np.zeros((len(train_tweet_embeddings), 300))\n",
        "\n",
        "for i in range(len(train_tweet_embeddings)):\n",
        "  np_train_tweet_embeddings[i,:] = np.array(train_tweet_embeddings[i])\n",
        "\n",
        "np_train_tweet_embeddings.shape\n",
        "\n",
        "np_test_tweet_embeddings = np.zeros((len(test_tweet_embedding), 300))\n",
        "\n",
        "for i in range(len(test_tweet_embedding)):\n",
        "  np_test_tweet_embeddings[i,:] = np.array(test_tweet_embedding[i])\n",
        "\n",
        "np_train_tweet_embeddings.shape\n",
        "np_test_tweet_embeddings.shape\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1869, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgBUpgkBWXES",
        "colab_type": "text"
      },
      "source": [
        "Now we will further analyse the new Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R36heWfu8vlr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5004377f-03e2-4f8f-97aa-6d794da31c0d"
      },
      "source": [
        "label_v = {'negative':0, 'neutral':1, 'positive':2}\n",
        "y_train = np.array([label_v[i] for i in train_label])\n",
        "y_test = np.array([label_v[i] for i in test_label])\n",
        "\n",
        "max_features = 20000\n",
        "tokenizer1 = Tokenizer(num_words=max_features)\n",
        "tokenizer1.fit_on_texts(train_tweet)\n",
        "\n",
        "max_len = 300\n",
        "num_classes = 3\n",
        "\n",
        "sequences_train = tokenizer1.texts_to_sequences(train_tweet)\n",
        "sequences_test = tokenizer1.texts_to_sequences(test_tweet)\n",
        "X_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
        "X_test = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
        "X_train_1=np.hstack((X_train, np_train_tweet_embeddings))\n",
        "X_test_1=np.hstack((X_test, np_test_tweet_embeddings))\n",
        "\n",
        "#X_train=append(X_train_1,train_tweet_embeddings, axis=1)\n",
        "  \n",
        "#X_test=append(X_test_1,test_tweet_embedding, axis=1)\n",
        "  \n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print('X_train shape:', X_train_1.shape)\n",
        "print('X_test shape:', X_test_1.shape)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (15131, 600)\n",
            "X_test shape: (1869, 600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fniv7ajs_NiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "nb_filter = 300\n",
        "filter_length = 3\n",
        "hidden_dims = 300 # 250\n",
        "nb_epoch = 2\n",
        "\n",
        "\n",
        "cmodel1 = Sequential()\n",
        "cmodel1.add(Dense(600))\n",
        "cmodel1.add(Dropout(0.2))\n",
        "# we add a Convolution1D, which will learn nb_filter\n",
        "# word group filters of size filter_length:\n",
        "#cmodel1.add(Convolution1D(nb_filter=nb_filter,\n",
        "#                       filter_length=filter_length,\n",
        "#                       border_mode='valid',\n",
        "#                       activation='tanh',\n",
        "#                       subsample_length=1))\n",
        "\n",
        "#cmodel1.add(BatchNormalization())\n",
        "from keras import optimizers\n",
        "def max_1d(X):\n",
        "    return K.max(X, axis=1)\n",
        "\n",
        "#cmodel1.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
        "cmodel1.add(Dense(hidden_dims))\n",
        "cmodel1.add(Dropout(0.2))\n",
        "cmodel1.add(Activation('relu'))\n",
        "cmodel1.add(Dense(num_classes))\n",
        "cmodel1.add(Activation('sigmoid'))\n",
        "adam = optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "cmodel1.compile(loss='binary_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvuPTJGU_R-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "bfa0bbd2-311e-4ba3-8a48-93d1924fa9c3"
      },
      "source": [
        "cmodel1.fit(X_train_1, Y_train, epochs = 2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "15131/15131 [==============================] - 6s 369us/step - loss: nan - acc: 0.0022\n",
            "Epoch 2/2\n",
            "15131/15131 [==============================] - 5s 353us/step - loss: nan - acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb22d55e278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2orSKEh0_VPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fdb7640-a3c3-4f86-a967-4b3c11fbb444"
      },
      "source": [
        "predsc1 = cmodel1.predict_classes(X_test_1, verbose=0)\n",
        "np.sum(predsc1==y_test)/len(y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28517924023542"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Ux-vim_X4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dde560b-c928-4ce2-c253-6f97c8ffc729"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as p_recall_f\n",
        "p_recall_f(y_test, predsc1, average='micro')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5564472980203318, 0.5564472980203318, 0.5564472980203318, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyCzAjJO_a7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c47aa1e-39db-4eac-da6a-76df2d81c1d6"
      },
      "source": [
        "\n",
        "p_recall_f(y_test, predsc1, average='macro')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5570606144166599, 0.5709277653907551, 0.5591825497916824, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LprgKl93_dBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6853d71-2206-43b3-8a8c-5eed115df431"
      },
      "source": [
        "p_recall_f(y_test, predsc1, average='weighted')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.55634354185309, 0.5564472980203318, 0.551201611107008, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgYLe2ZO_gKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4fcdde1e-95b9-4950-e046-78f6080995bb"
      },
      "source": [
        "\n",
        "cmodel1.summary()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 300)         6000000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 300)         270300    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 6,361,503\n",
            "Trainable params: 6,361,503\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rGPBfqmLrw9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}